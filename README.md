# DeepLip
deep-learning based audio-visual lip bometrics
![avlip](https://user-images.githubusercontent.com/45690014/115110548-95209b00-9fae-11eb-9fec-1c47e85afb5d.png)



# Author
 Meng Liu, Chang Zeng, Hanyi Zhang
 e-mail: liumeng2017@tju.edu.cn

# Content
This work has been submitted to Interspeech2021. It introduces a deep-learning based audio-visual lip biometrics framework, illustrated as the above figure. Since the audio-visual lip biometrics study has been hindered by the lack of appropriate and sizeable database, this work presents a moderate baseline database using public lip datasets, as well as the baseline system. 

Different from other audio-visual speaker recognition methods, audio-visual lip biometrics is interesting. It leverages the multimodal information from the audible speech and visual speech (i.e., lip movements). Many work hasn't been explored in this area. We will update the code and resource as the advancement of our process.

# Have Done
* establish a public DeepLip database as well as a well performed baseline system
* prove the feasibility of deep-learning based audio-visual lip biometrics
* show the complementary power of fusing the audible speech and visual speech 

# To Do List
* complex multimodal fusion methods
* compared with other audio-visual speaker recognition methods
* prove the robustness of spoof and noisy enviroments
* text-dependent audio-visual lip biometrics
* collect large audio-visual lip database

# Cite
